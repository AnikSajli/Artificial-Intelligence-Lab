{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HouseRentWithNeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoAazqLgcUfb"
      },
      "source": [
        "#import libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn import linear_model\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovZSQSrycuG0"
      },
      "source": [
        "# load boston house rent data set from sklearen dataset\r\n",
        "from sklearn.datasets import load_boston\r\n",
        "boston = load_boston()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M68EY0Fyc1Kl",
        "outputId": "f2f29004-9edf-41ba-e525-a9a1e11546ef"
      },
      "source": [
        "dataFrame_X = pd.DataFrame(boston.data, columns= boston.feature_names)\r\n",
        "print(dataFrame_X.head(5))\r\n",
        "dataFrame_Y = pd.DataFrame(boston.target)\r\n",
        "print(dataFrame_Y.head(5))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n",
            "0  0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.90   4.98\n",
            "1  0.02731   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  396.90   9.14\n",
            "2  0.02729   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  392.83   4.03\n",
            "3  0.03237   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  394.63   2.94\n",
            "4  0.06905   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  396.90   5.33\n",
            "\n",
            "[5 rows x 13 columns]\n",
            "      0\n",
            "0  24.0\n",
            "1  21.6\n",
            "2  34.7\n",
            "3  33.4\n",
            "4  36.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "uAA5mwsGdJE7",
        "outputId": "fa5121e3-b79e-4f7c-99fb-f2e035789ce8"
      },
      "source": [
        "dataFrame_X.corr()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>CRIM</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.200469</td>\n",
              "      <td>0.406583</td>\n",
              "      <td>-0.055892</td>\n",
              "      <td>0.420972</td>\n",
              "      <td>-0.219247</td>\n",
              "      <td>0.352734</td>\n",
              "      <td>-0.379670</td>\n",
              "      <td>0.625505</td>\n",
              "      <td>0.582764</td>\n",
              "      <td>0.289946</td>\n",
              "      <td>-0.385064</td>\n",
              "      <td>0.455621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ZN</th>\n",
              "      <td>-0.200469</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.533828</td>\n",
              "      <td>-0.042697</td>\n",
              "      <td>-0.516604</td>\n",
              "      <td>0.311991</td>\n",
              "      <td>-0.569537</td>\n",
              "      <td>0.664408</td>\n",
              "      <td>-0.311948</td>\n",
              "      <td>-0.314563</td>\n",
              "      <td>-0.391679</td>\n",
              "      <td>0.175520</td>\n",
              "      <td>-0.412995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>INDUS</th>\n",
              "      <td>0.406583</td>\n",
              "      <td>-0.533828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.062938</td>\n",
              "      <td>0.763651</td>\n",
              "      <td>-0.391676</td>\n",
              "      <td>0.644779</td>\n",
              "      <td>-0.708027</td>\n",
              "      <td>0.595129</td>\n",
              "      <td>0.720760</td>\n",
              "      <td>0.383248</td>\n",
              "      <td>-0.356977</td>\n",
              "      <td>0.603800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CHAS</th>\n",
              "      <td>-0.055892</td>\n",
              "      <td>-0.042697</td>\n",
              "      <td>0.062938</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.091203</td>\n",
              "      <td>0.091251</td>\n",
              "      <td>0.086518</td>\n",
              "      <td>-0.099176</td>\n",
              "      <td>-0.007368</td>\n",
              "      <td>-0.035587</td>\n",
              "      <td>-0.121515</td>\n",
              "      <td>0.048788</td>\n",
              "      <td>-0.053929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NOX</th>\n",
              "      <td>0.420972</td>\n",
              "      <td>-0.516604</td>\n",
              "      <td>0.763651</td>\n",
              "      <td>0.091203</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.302188</td>\n",
              "      <td>0.731470</td>\n",
              "      <td>-0.769230</td>\n",
              "      <td>0.611441</td>\n",
              "      <td>0.668023</td>\n",
              "      <td>0.188933</td>\n",
              "      <td>-0.380051</td>\n",
              "      <td>0.590879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RM</th>\n",
              "      <td>-0.219247</td>\n",
              "      <td>0.311991</td>\n",
              "      <td>-0.391676</td>\n",
              "      <td>0.091251</td>\n",
              "      <td>-0.302188</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.240265</td>\n",
              "      <td>0.205246</td>\n",
              "      <td>-0.209847</td>\n",
              "      <td>-0.292048</td>\n",
              "      <td>-0.355501</td>\n",
              "      <td>0.128069</td>\n",
              "      <td>-0.613808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGE</th>\n",
              "      <td>0.352734</td>\n",
              "      <td>-0.569537</td>\n",
              "      <td>0.644779</td>\n",
              "      <td>0.086518</td>\n",
              "      <td>0.731470</td>\n",
              "      <td>-0.240265</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.747881</td>\n",
              "      <td>0.456022</td>\n",
              "      <td>0.506456</td>\n",
              "      <td>0.261515</td>\n",
              "      <td>-0.273534</td>\n",
              "      <td>0.602339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DIS</th>\n",
              "      <td>-0.379670</td>\n",
              "      <td>0.664408</td>\n",
              "      <td>-0.708027</td>\n",
              "      <td>-0.099176</td>\n",
              "      <td>-0.769230</td>\n",
              "      <td>0.205246</td>\n",
              "      <td>-0.747881</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.494588</td>\n",
              "      <td>-0.534432</td>\n",
              "      <td>-0.232471</td>\n",
              "      <td>0.291512</td>\n",
              "      <td>-0.496996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RAD</th>\n",
              "      <td>0.625505</td>\n",
              "      <td>-0.311948</td>\n",
              "      <td>0.595129</td>\n",
              "      <td>-0.007368</td>\n",
              "      <td>0.611441</td>\n",
              "      <td>-0.209847</td>\n",
              "      <td>0.456022</td>\n",
              "      <td>-0.494588</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.910228</td>\n",
              "      <td>0.464741</td>\n",
              "      <td>-0.444413</td>\n",
              "      <td>0.488676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TAX</th>\n",
              "      <td>0.582764</td>\n",
              "      <td>-0.314563</td>\n",
              "      <td>0.720760</td>\n",
              "      <td>-0.035587</td>\n",
              "      <td>0.668023</td>\n",
              "      <td>-0.292048</td>\n",
              "      <td>0.506456</td>\n",
              "      <td>-0.534432</td>\n",
              "      <td>0.910228</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.460853</td>\n",
              "      <td>-0.441808</td>\n",
              "      <td>0.543993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PTRATIO</th>\n",
              "      <td>0.289946</td>\n",
              "      <td>-0.391679</td>\n",
              "      <td>0.383248</td>\n",
              "      <td>-0.121515</td>\n",
              "      <td>0.188933</td>\n",
              "      <td>-0.355501</td>\n",
              "      <td>0.261515</td>\n",
              "      <td>-0.232471</td>\n",
              "      <td>0.464741</td>\n",
              "      <td>0.460853</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.177383</td>\n",
              "      <td>0.374044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B</th>\n",
              "      <td>-0.385064</td>\n",
              "      <td>0.175520</td>\n",
              "      <td>-0.356977</td>\n",
              "      <td>0.048788</td>\n",
              "      <td>-0.380051</td>\n",
              "      <td>0.128069</td>\n",
              "      <td>-0.273534</td>\n",
              "      <td>0.291512</td>\n",
              "      <td>-0.444413</td>\n",
              "      <td>-0.441808</td>\n",
              "      <td>-0.177383</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.366087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LSTAT</th>\n",
              "      <td>0.455621</td>\n",
              "      <td>-0.412995</td>\n",
              "      <td>0.603800</td>\n",
              "      <td>-0.053929</td>\n",
              "      <td>0.590879</td>\n",
              "      <td>-0.613808</td>\n",
              "      <td>0.602339</td>\n",
              "      <td>-0.496996</td>\n",
              "      <td>0.488676</td>\n",
              "      <td>0.543993</td>\n",
              "      <td>0.374044</td>\n",
              "      <td>-0.366087</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             CRIM        ZN     INDUS  ...   PTRATIO         B     LSTAT\n",
              "CRIM     1.000000 -0.200469  0.406583  ...  0.289946 -0.385064  0.455621\n",
              "ZN      -0.200469  1.000000 -0.533828  ... -0.391679  0.175520 -0.412995\n",
              "INDUS    0.406583 -0.533828  1.000000  ...  0.383248 -0.356977  0.603800\n",
              "CHAS    -0.055892 -0.042697  0.062938  ... -0.121515  0.048788 -0.053929\n",
              "NOX      0.420972 -0.516604  0.763651  ...  0.188933 -0.380051  0.590879\n",
              "RM      -0.219247  0.311991 -0.391676  ... -0.355501  0.128069 -0.613808\n",
              "AGE      0.352734 -0.569537  0.644779  ...  0.261515 -0.273534  0.602339\n",
              "DIS     -0.379670  0.664408 -0.708027  ... -0.232471  0.291512 -0.496996\n",
              "RAD      0.625505 -0.311948  0.595129  ...  0.464741 -0.444413  0.488676\n",
              "TAX      0.582764 -0.314563  0.720760  ...  0.460853 -0.441808  0.543993\n",
              "PTRATIO  0.289946 -0.391679  0.383248  ...  1.000000 -0.177383  0.374044\n",
              "B       -0.385064  0.175520 -0.356977  ... -0.177383  1.000000 -0.366087\n",
              "LSTAT    0.455621 -0.412995  0.603800  ...  0.374044 -0.366087  1.000000\n",
              "\n",
              "[13 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F29JDJGSc8aD"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(dataFrame_X,dataFrame_Y,test_size = 0.33,random_state=42)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-l7pBIAdZzb"
      },
      "source": [
        "#import the libraries\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LeakyReLU,PReLU,ELU\r\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYqn_dRRdkgK",
        "outputId": "486f34eb-0304-4b07-9ab1-3f8b26a7a5ad"
      },
      "source": [
        "#make the model\r\n",
        "neuralNetworkModel = Sequential()\r\n",
        "\r\n",
        "# The Input Layer :\r\n",
        "neuralNetworkModel.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\r\n",
        "\r\n",
        "# The Hidden Layers :\r\n",
        "neuralNetworkModel.add(Dense(256, kernel_initializer='normal',activation='relu'))\r\n",
        "neuralNetworkModel.add(Dense(256, kernel_initializer='normal',activation='relu'))\r\n",
        "neuralNetworkModel.add(Dense(256, kernel_initializer='normal',activation='relu'))\r\n",
        "neuralNetworkModel.add(Dense(256, kernel_initializer='normal',activation='relu'))\r\n",
        "neuralNetworkModel.add(Dense(256, kernel_initializer='normal',activation='relu'))\r\n",
        "\r\n",
        "# The Output Layer :\r\n",
        "neuralNetworkModel.add(Dense(1, kernel_initializer='normal',activation='linear'))\r\n",
        "\r\n",
        "# Compile the network :\r\n",
        "neuralNetworkModel.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\r\n",
        "neuralNetworkModel.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 128)               1792      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 298,241\n",
            "Trainable params: 298,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTmmoId2dn5M",
        "outputId": "feb4c38f-6dc0-45a8-d376-8c569ffac4ac"
      },
      "source": [
        "neuralNetworkModel.fit(X_train, Y_train,validation_split=0.33, batch_size = 10, epochs = 100)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "23/23 [==============================] - 1s 17ms/step - loss: 14.5471 - mean_absolute_error: 14.5471 - val_loss: 8.9359 - val_mean_absolute_error: 8.9359\n",
            "Epoch 2/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.8474 - mean_absolute_error: 7.8474 - val_loss: 5.1558 - val_mean_absolute_error: 5.1558\n",
            "Epoch 3/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.0162 - mean_absolute_error: 7.0162 - val_loss: 5.7562 - val_mean_absolute_error: 5.7562\n",
            "Epoch 4/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 6.4090 - mean_absolute_error: 6.4090 - val_loss: 5.5322 - val_mean_absolute_error: 5.5322\n",
            "Epoch 5/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.7226 - mean_absolute_error: 5.7226 - val_loss: 10.8670 - val_mean_absolute_error: 10.8670\n",
            "Epoch 6/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 7.1911 - mean_absolute_error: 7.1911 - val_loss: 5.3339 - val_mean_absolute_error: 5.3339\n",
            "Epoch 7/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.1133 - mean_absolute_error: 5.1133 - val_loss: 5.1985 - val_mean_absolute_error: 5.1985\n",
            "Epoch 8/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.1228 - mean_absolute_error: 6.1228 - val_loss: 5.3537 - val_mean_absolute_error: 5.3537\n",
            "Epoch 9/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.6569 - mean_absolute_error: 5.6569 - val_loss: 4.8119 - val_mean_absolute_error: 4.8119\n",
            "Epoch 10/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.4591 - mean_absolute_error: 5.4591 - val_loss: 7.0838 - val_mean_absolute_error: 7.0838\n",
            "Epoch 11/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 6.7180 - mean_absolute_error: 6.7180 - val_loss: 5.9253 - val_mean_absolute_error: 5.9253\n",
            "Epoch 12/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 6.0299 - mean_absolute_error: 6.0299 - val_loss: 5.1985 - val_mean_absolute_error: 5.1985\n",
            "Epoch 13/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 5.7011 - mean_absolute_error: 5.7011 - val_loss: 5.1305 - val_mean_absolute_error: 5.1305\n",
            "Epoch 14/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.2165 - mean_absolute_error: 5.2165 - val_loss: 5.0810 - val_mean_absolute_error: 5.0810\n",
            "Epoch 15/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.1984 - mean_absolute_error: 5.1984 - val_loss: 5.8504 - val_mean_absolute_error: 5.8504\n",
            "Epoch 16/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.0482 - mean_absolute_error: 5.0482 - val_loss: 4.9667 - val_mean_absolute_error: 4.9667\n",
            "Epoch 17/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.7511 - mean_absolute_error: 5.7511 - val_loss: 5.3190 - val_mean_absolute_error: 5.3190\n",
            "Epoch 18/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.9818 - mean_absolute_error: 4.9818 - val_loss: 4.5595 - val_mean_absolute_error: 4.5595\n",
            "Epoch 19/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.1262 - mean_absolute_error: 5.1262 - val_loss: 4.6186 - val_mean_absolute_error: 4.6186\n",
            "Epoch 20/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.4934 - mean_absolute_error: 5.4934 - val_loss: 4.9931 - val_mean_absolute_error: 4.9931\n",
            "Epoch 21/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.1338 - mean_absolute_error: 5.1338 - val_loss: 4.5311 - val_mean_absolute_error: 4.5311\n",
            "Epoch 22/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.1596 - mean_absolute_error: 5.1596 - val_loss: 6.1561 - val_mean_absolute_error: 6.1561\n",
            "Epoch 23/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.9790 - mean_absolute_error: 4.9790 - val_loss: 7.2689 - val_mean_absolute_error: 7.2689\n",
            "Epoch 24/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.6330 - mean_absolute_error: 5.6330 - val_loss: 5.3474 - val_mean_absolute_error: 5.3474\n",
            "Epoch 25/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.8216 - mean_absolute_error: 4.8216 - val_loss: 4.8050 - val_mean_absolute_error: 4.8050\n",
            "Epoch 26/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.3130 - mean_absolute_error: 5.3130 - val_loss: 4.4028 - val_mean_absolute_error: 4.4028\n",
            "Epoch 27/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.8074 - mean_absolute_error: 4.8074 - val_loss: 4.5541 - val_mean_absolute_error: 4.5541\n",
            "Epoch 28/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.2712 - mean_absolute_error: 4.2712 - val_loss: 5.7222 - val_mean_absolute_error: 5.7222\n",
            "Epoch 29/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.0738 - mean_absolute_error: 5.0738 - val_loss: 4.9377 - val_mean_absolute_error: 4.9377\n",
            "Epoch 30/100\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.8069 - mean_absolute_error: 4.8069 - val_loss: 4.7105 - val_mean_absolute_error: 4.7105\n",
            "Epoch 31/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.8942 - mean_absolute_error: 4.8942 - val_loss: 4.2664 - val_mean_absolute_error: 4.2664\n",
            "Epoch 32/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.1257 - mean_absolute_error: 4.1257 - val_loss: 4.0389 - val_mean_absolute_error: 4.0389\n",
            "Epoch 33/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.8852 - mean_absolute_error: 3.8852 - val_loss: 4.1336 - val_mean_absolute_error: 4.1336\n",
            "Epoch 34/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0045 - mean_absolute_error: 4.0045 - val_loss: 5.2480 - val_mean_absolute_error: 5.2480\n",
            "Epoch 35/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.1649 - mean_absolute_error: 4.1649 - val_loss: 5.7411 - val_mean_absolute_error: 5.7411\n",
            "Epoch 36/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.6253 - mean_absolute_error: 4.6253 - val_loss: 4.0020 - val_mean_absolute_error: 4.0020\n",
            "Epoch 37/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.2587 - mean_absolute_error: 4.2587 - val_loss: 3.9269 - val_mean_absolute_error: 3.9269\n",
            "Epoch 38/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.8961 - mean_absolute_error: 3.8961 - val_loss: 4.7610 - val_mean_absolute_error: 4.7610\n",
            "Epoch 39/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.2130 - mean_absolute_error: 5.2130 - val_loss: 3.9583 - val_mean_absolute_error: 3.9583\n",
            "Epoch 40/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.5452 - mean_absolute_error: 3.5452 - val_loss: 3.7396 - val_mean_absolute_error: 3.7396\n",
            "Epoch 41/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.2487 - mean_absolute_error: 4.2487 - val_loss: 4.0004 - val_mean_absolute_error: 4.0004\n",
            "Epoch 42/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.8798 - mean_absolute_error: 3.8798 - val_loss: 3.7210 - val_mean_absolute_error: 3.7210\n",
            "Epoch 43/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.4860 - mean_absolute_error: 4.4860 - val_loss: 6.0944 - val_mean_absolute_error: 6.0944\n",
            "Epoch 44/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.7519 - mean_absolute_error: 4.7519 - val_loss: 3.8095 - val_mean_absolute_error: 3.8095\n",
            "Epoch 45/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.1683 - mean_absolute_error: 4.1683 - val_loss: 4.8900 - val_mean_absolute_error: 4.8900\n",
            "Epoch 46/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 3.7729 - mean_absolute_error: 3.7729 - val_loss: 3.5964 - val_mean_absolute_error: 3.5964\n",
            "Epoch 47/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.9791 - mean_absolute_error: 3.9791 - val_loss: 3.4228 - val_mean_absolute_error: 3.4228\n",
            "Epoch 48/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.8400 - mean_absolute_error: 3.8400 - val_loss: 5.0850 - val_mean_absolute_error: 5.0850\n",
            "Epoch 49/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.4151 - mean_absolute_error: 4.4151 - val_loss: 3.4427 - val_mean_absolute_error: 3.4427\n",
            "Epoch 50/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.2801 - mean_absolute_error: 3.2801 - val_loss: 3.4406 - val_mean_absolute_error: 3.4406\n",
            "Epoch 51/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.7433 - mean_absolute_error: 3.7433 - val_loss: 3.5010 - val_mean_absolute_error: 3.5010\n",
            "Epoch 52/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9132 - mean_absolute_error: 3.9132 - val_loss: 3.2604 - val_mean_absolute_error: 3.2604\n",
            "Epoch 53/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.3827 - mean_absolute_error: 3.3827 - val_loss: 3.3296 - val_mean_absolute_error: 3.3296\n",
            "Epoch 54/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.6368 - mean_absolute_error: 3.6368 - val_loss: 3.6880 - val_mean_absolute_error: 3.6880\n",
            "Epoch 55/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.1431 - mean_absolute_error: 3.1431 - val_loss: 3.4836 - val_mean_absolute_error: 3.4836\n",
            "Epoch 56/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.3798 - mean_absolute_error: 4.3798 - val_loss: 3.5670 - val_mean_absolute_error: 3.5670\n",
            "Epoch 57/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.3662 - mean_absolute_error: 3.3662 - val_loss: 3.6178 - val_mean_absolute_error: 3.6178\n",
            "Epoch 58/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.6183 - mean_absolute_error: 3.6183 - val_loss: 3.3340 - val_mean_absolute_error: 3.3340\n",
            "Epoch 59/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3112 - mean_absolute_error: 3.3112 - val_loss: 3.4593 - val_mean_absolute_error: 3.4593\n",
            "Epoch 60/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1626 - mean_absolute_error: 3.1626 - val_loss: 3.5843 - val_mean_absolute_error: 3.5843\n",
            "Epoch 61/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.1085 - mean_absolute_error: 3.1085 - val_loss: 2.9539 - val_mean_absolute_error: 2.9539\n",
            "Epoch 62/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.5242 - mean_absolute_error: 3.5242 - val_loss: 3.3278 - val_mean_absolute_error: 3.3278\n",
            "Epoch 63/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 2.9450 - mean_absolute_error: 2.9450 - val_loss: 3.8700 - val_mean_absolute_error: 3.8700\n",
            "Epoch 64/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2215 - mean_absolute_error: 3.2215 - val_loss: 4.5751 - val_mean_absolute_error: 4.5751\n",
            "Epoch 65/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.7269 - mean_absolute_error: 3.7269 - val_loss: 3.8426 - val_mean_absolute_error: 3.8426\n",
            "Epoch 66/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.4291 - mean_absolute_error: 3.4291 - val_loss: 3.3497 - val_mean_absolute_error: 3.3497\n",
            "Epoch 67/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.3432 - mean_absolute_error: 3.3432 - val_loss: 3.1052 - val_mean_absolute_error: 3.1052\n",
            "Epoch 68/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.8763 - mean_absolute_error: 2.8763 - val_loss: 3.9774 - val_mean_absolute_error: 3.9774\n",
            "Epoch 69/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 4.1845 - mean_absolute_error: 4.1845 - val_loss: 3.2002 - val_mean_absolute_error: 3.2002\n",
            "Epoch 70/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3001 - mean_absolute_error: 3.3001 - val_loss: 3.0736 - val_mean_absolute_error: 3.0736\n",
            "Epoch 71/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.3771 - mean_absolute_error: 4.3771 - val_loss: 3.3905 - val_mean_absolute_error: 3.3905\n",
            "Epoch 72/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.2020 - mean_absolute_error: 3.2020 - val_loss: 4.6051 - val_mean_absolute_error: 4.6051\n",
            "Epoch 73/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.7902 - mean_absolute_error: 3.7902 - val_loss: 4.0877 - val_mean_absolute_error: 4.0877\n",
            "Epoch 74/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.9524 - mean_absolute_error: 3.9524 - val_loss: 3.0832 - val_mean_absolute_error: 3.0832\n",
            "Epoch 75/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 2.9689 - mean_absolute_error: 2.9689 - val_loss: 4.2126 - val_mean_absolute_error: 4.2126\n",
            "Epoch 76/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.4978 - mean_absolute_error: 3.4978 - val_loss: 3.0954 - val_mean_absolute_error: 3.0954\n",
            "Epoch 77/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7204 - mean_absolute_error: 2.7204 - val_loss: 3.0800 - val_mean_absolute_error: 3.0800\n",
            "Epoch 78/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0476 - mean_absolute_error: 3.0476 - val_loss: 3.2399 - val_mean_absolute_error: 3.2399\n",
            "Epoch 79/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.4187 - mean_absolute_error: 4.4187 - val_loss: 4.4460 - val_mean_absolute_error: 4.4460\n",
            "Epoch 80/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2933 - mean_absolute_error: 3.2933 - val_loss: 3.7279 - val_mean_absolute_error: 3.7279\n",
            "Epoch 81/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 3.4556 - mean_absolute_error: 3.4556 - val_loss: 3.7654 - val_mean_absolute_error: 3.7654\n",
            "Epoch 82/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.2045 - mean_absolute_error: 3.2045 - val_loss: 3.0587 - val_mean_absolute_error: 3.0587\n",
            "Epoch 83/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.5595 - mean_absolute_error: 2.5595 - val_loss: 3.5933 - val_mean_absolute_error: 3.5933\n",
            "Epoch 84/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.0181 - mean_absolute_error: 3.0181 - val_loss: 3.2261 - val_mean_absolute_error: 3.2261\n",
            "Epoch 85/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.1481 - mean_absolute_error: 3.1481 - val_loss: 3.0019 - val_mean_absolute_error: 3.0019\n",
            "Epoch 86/100\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.5384 - mean_absolute_error: 3.5384 - val_loss: 3.6808 - val_mean_absolute_error: 3.6808\n",
            "Epoch 87/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.1249 - mean_absolute_error: 3.1249 - val_loss: 3.7514 - val_mean_absolute_error: 3.7514\n",
            "Epoch 88/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1438 - mean_absolute_error: 3.1438 - val_loss: 2.9792 - val_mean_absolute_error: 2.9792\n",
            "Epoch 89/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.8254 - mean_absolute_error: 2.8254 - val_loss: 2.8797 - val_mean_absolute_error: 2.8797\n",
            "Epoch 90/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 2.8258 - mean_absolute_error: 2.8258 - val_loss: 3.1608 - val_mean_absolute_error: 3.1608\n",
            "Epoch 91/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 2.8688 - mean_absolute_error: 2.8688 - val_loss: 3.0060 - val_mean_absolute_error: 3.0060\n",
            "Epoch 92/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.9382 - mean_absolute_error: 2.9382 - val_loss: 3.4500 - val_mean_absolute_error: 3.4500\n",
            "Epoch 93/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.1387 - mean_absolute_error: 3.1387 - val_loss: 3.3178 - val_mean_absolute_error: 3.3178\n",
            "Epoch 94/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.1628 - mean_absolute_error: 3.1628 - val_loss: 3.4128 - val_mean_absolute_error: 3.4128\n",
            "Epoch 95/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.1194 - mean_absolute_error: 3.1194 - val_loss: 3.1078 - val_mean_absolute_error: 3.1078\n",
            "Epoch 96/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.3831 - mean_absolute_error: 3.3831 - val_loss: 4.7436 - val_mean_absolute_error: 4.7436\n",
            "Epoch 97/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.9140 - mean_absolute_error: 3.9140 - val_loss: 3.5664 - val_mean_absolute_error: 3.5664\n",
            "Epoch 98/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.4105 - mean_absolute_error: 3.4105 - val_loss: 3.0211 - val_mean_absolute_error: 3.0211\n",
            "Epoch 99/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 3.0213 - mean_absolute_error: 3.0213 - val_loss: 3.3884 - val_mean_absolute_error: 3.3884\n",
            "Epoch 100/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.8132 - mean_absolute_error: 2.8132 - val_loss: 2.9860 - val_mean_absolute_error: 2.9860\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f69149499b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cZBD3wXd3c9"
      },
      "source": [
        "predictValues = neuralNetworkModel.predict(X_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "pu36YPDPeMzK",
        "outputId": "4917e1b2-0b98-4161-d72c-2a29adfb5573"
      },
      "source": [
        "plt.scatter(Y_test,predictValues)\r\n",
        "from sklearn import metrics\r\n",
        "print('MAE:', metrics.mean_absolute_error(Y_test, predictValues))\r\n",
        "print('MSE:', metrics.mean_squared_error(Y_test, predictValues))\r\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, predictValues)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE: 2.7678165647084128\n",
            "MSE: 18.866026859755806\n",
            "RMSE: 4.343503984084256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}